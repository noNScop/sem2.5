{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/07_pytorch_experiment_tracking_exercise_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNqPNlYylluR"
   },
   "source": [
    "# 07. PyTorch Experiment Tracking Exercise Template\n",
    "\n",
    "Welcome to the 07. PyTorch Experiment Tracking exercise template notebook.\n",
    "\n",
    "> **Note:** There may be more than one solution to each of the exercises. This notebook only shows one possible example.\n",
    "\n",
    "## Resources\n",
    "\n",
    "1. These exercises/solutions are based on [section 07. PyTorch Transfer Learning](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) of the Learn PyTorch for Deep Learning course by Zero to Mastery.\n",
    "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/cO_r2FYcAjU).\n",
    "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions).\n",
    "\n",
    "> **Note:** The first section of this notebook is dedicated to getting various helper functions and datasets used for the exercises. The exercises start at the heading \"Exercise 1: ...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf8ab9cyHTzU"
   },
   "source": [
    "### Get various imports and helper functions\n",
    "\n",
    "We'll need to make sure we have `torch` v.1.12+ and `torchvision` v0.13+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MOv1De4mxeL",
    "outputId": "a4f9fdd9-f225-4861-e204-e26186365bea"
   },
   "outputs": [],
   "source": [
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "# try:\n",
    "#     import torch\n",
    "#     import torchvision\n",
    "#     assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
    "#     assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
    "#     print(f\"torch version: {torch.__version__}\")\n",
    "#     print(f\"torchvision version: {torchvision.__version__}\")\n",
    "# except:\n",
    "#     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "#     !pip3 install -U --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
    "#     import torch\n",
    "#     import torchvision\n",
    "#     print(f\"torch version: {torch.__version__}\")\n",
    "#     print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Nf-DsrZipCE9",
    "outputId": "f4cc7d1c-da78-4eb4-c753-4e135771650c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Make sure we have a GPU\n",
    " device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    " device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i_52puIeoab3"
   },
   "outputs": [],
   "source": [
    "# Get regular imports \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "# Try to get torchinfo, install it if it doesn't work\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except:\n",
    "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
    "    !pip install -q torchinfo\n",
    "    from torchinfo import summary\n",
    "\n",
    "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
    "try:\n",
    "    from going_modular.going_modular import data_setup, engine\n",
    "except:\n",
    "    # Get the going_modular scripts\n",
    "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
    "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
    "    !mv pytorch-deep-learning/going_modular .\n",
    "    !rm -rf pytorch-deep-learning\n",
    "    from going_modular.going_modular import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DBj8I3P9pNK2"
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # Set the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6R-CS53pTLS",
    "outputId": "3f7688b7-0b86-4cd8-bb11-4e71f8e7270f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def download_data(source: str, \n",
    "                  destination: str,\n",
    "                  remove_source: bool = True) -> Path:\n",
    "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
    "\n",
    "    Args:\n",
    "        source (str): A link to a zipped file containing data.\n",
    "        destination (str): A target directory to unzip data to.\n",
    "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
    "    \n",
    "    Returns:\n",
    "        pathlib.Path to downloaded data.\n",
    "    \n",
    "    Example usage:\n",
    "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                      destination=\"pizza_steak_sushi\")\n",
    "    \"\"\"\n",
    "    # Setup path to data folder\n",
    "    data_path = Path(\"data/\")\n",
    "    image_path = data_path / destination\n",
    "\n",
    "    # If the image folder doesn't exist, download it and prepare it... \n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
    "        image_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download pizza, steak, sushi data\n",
    "        target_file = Path(source).name\n",
    "        with open(data_path / target_file, \"wb\") as f:\n",
    "            request = requests.get(source)\n",
    "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
    "            f.write(request.content)\n",
    "\n",
    "        # Unzip pizza, steak, sushi data\n",
    "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
    "            print(f\"[INFO] Unzipping {target_file} data...\") \n",
    "            zip_ref.extractall(image_path)\n",
    "\n",
    "        # Remove .zip file\n",
    "        if remove_source:\n",
    "            os.remove(data_path / target_file)\n",
    "    \n",
    "    return image_path\n",
    "\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BE60IEEkr89l"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str=None):\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0BH4ONGsgNB",
    "outputId": "077f7b39-f4d0-44dd-b74f-8ead04fb7add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/test_experiment_name/this_is_the_model_name/add_a_little_extra_if_you_want...\n"
     ]
    }
   ],
   "source": [
    "# Create a test writer\n",
    "writer = create_writer(experiment_name=\"test_experiment_name\",\n",
    "                       model_name=\"this_is_the_model_name\",\n",
    "                       extra=\"add_a_little_extra_if_you_want\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VwO0Q1eFsusV"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from going_modular.going_modular.engine import train_step, test_step\n",
    "\n",
    "# Add writer parameter to train()\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device, \n",
    "          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n",
    "          ) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Stores metrics to specified writer log_dir if present.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      writer: A SummaryWriter() instance to log model results to.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                train_acc: [...],\n",
    "                test_loss: [...],\n",
    "                test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                train_acc: [0.3945, 0.3945],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "                test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "        ### New: Use the writer parameter to track experiments ###\n",
    "        # See if there's a writer, if so, log to it\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(main_tag=\"Loss\", \n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc}, \n",
    "                               global_step=epoch)\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh8jKzHYHYL3"
   },
   "source": [
    "### Download data\n",
    "\n",
    "Using the same data from https://www.learnpytorch.io/07_pytorch_experiment_tracking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68QGCR_1tzif",
    "outputId": "a5073b19-1463-4d8a-ec08-5399945196a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download 10 percent and 20 percent training data (if necessary)\n",
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9L2rCRxvt1ED",
    "outputId": "30b8202e-96f3-443f-e89e-2a83e11b1c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory 10%: data/pizza_steak_sushi/train\n",
      "Training directory 20%: data/pizza_steak_sushi_20_percent/train\n",
      "Testing directory: data/pizza_steak_sushi/test\n"
     ]
    }
   ],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "# Check the directories\n",
    "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
    "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
    "print(f\"Testing directory: {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "K35q9wswt6NH"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Create a transform to normalize data distribution to be inline with ImageNet\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Create a transform pipeline\n",
    "simple_transform = transforms.Compose([\n",
    "                                       transforms.Resize((224, 224)),\n",
    "                                       transforms.ToTensor(), # get image values between 0 & 1\n",
    "                                       normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBuEla8pHea9"
   },
   "source": [
    "### Turn data into DataLoaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlQU94HBuqOq",
    "outputId": "db4b9144-fd3b-4f31-e0c1-ebd60f906232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 32 in 10 percent training data: 8\n",
      "Number of batches of size 32 in 20 percent training data: 15\n",
      "Number of batches of size 32 in testing data: 3 (all experiments will use the same test set)\n",
      "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test DataLoaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "                                                                                          test_dir=test_dir,\n",
    "                                                                                          transform=simple_transform,\n",
    "                                                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# Create 20% training and test DataLoaders\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                                                          test_dir=test_dir,\n",
    "                                                                                          transform=simple_transform,\n",
    "                                                                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
    "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(test_dataloader)} (all experiments will use the same test set)\")\n",
    "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwmoMhW8IqSu"
   },
   "source": [
    "## Exercise 1: Pick a larger model from [`torchvision.models`](https://pytorch.org/vision/main/models.html) to add to the list of experiments (for example, EffNetB3 or higher)\n",
    "\n",
    "* How does it perform compared to our existing models?\n",
    "* **Hint:** You'll need to set up an exerpiment similar to [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "# model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# for p in model.features.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Dropout(p=0.2, inplace=True),\n",
    "#     nn.Linear(1280, 3)\n",
    "# ).to(device)\n",
    "\n",
    "# summary(model, input_size=(BATCH_SIZE, 3, 224, 224), col_width=20, row_settings=[\"var_names\"],\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"]), model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb0():\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "    \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(1280, 3)\n",
    "    ).to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "# model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "# for p in model.features.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Dropout(p=0.3, inplace=True),\n",
    "#     nn.Linear(1408, 3)\n",
    "# ).to(device)\n",
    "\n",
    "# summary(model, input_size=(BATCH_SIZE, 3, 224, 224), col_width=20, row_settings=[\"var_names\"],\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"]), model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb2():\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "    \n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(1408, 3)\n",
    "    ).to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torchvision.models.EfficientNet_B3_Weights.DEFAULT\n",
    "# model = torchvision.models.efficientnet_b3(weights=weights).to(device)\n",
    "\n",
    "# for p in model.features.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Dropout(p=0.3, inplace=True),\n",
    "#     nn.Linear(1536, 3)\n",
    "# ).to(device)\n",
    "\n",
    "# summary(model, input_size=(BATCH_SIZE, 3, 224, 224), col_width=20, row_settings=[\"var_names\"],\n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"]), model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_effnetb3():\n",
    "    weights = torchvision.models.EfficientNet_B3_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b3(weights=weights).to(device)\n",
    "\n",
    "    for p in model.features.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(1536, 3)\n",
    "    ).to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = [(\"data_10_percent\", train_dataloader_10_percent), (\"data_20_percent\", train_dataloader_20_percent)]\n",
    "epochs_count = [5, 10]\n",
    "model_names = [\"effnetb0\", \"effnetb2\", \"effnetb3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F-35y0uxJ8tg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment numer: 1\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dd7e83741e4414a0393338ac405a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0108 | train_acc: 0.5234 | test_loss: 0.8805 | test_acc: 0.6723\n",
      "Epoch: 2 | train_loss: 0.9525 | train_acc: 0.4961 | test_loss: 0.8830 | test_acc: 0.5379\n",
      "Epoch: 3 | train_loss: 0.7442 | train_acc: 0.7930 | test_loss: 0.6465 | test_acc: 0.8864\n",
      "Epoch: 4 | train_loss: 0.7187 | train_acc: 0.7109 | test_loss: 0.5875 | test_acc: 0.8343\n",
      "Epoch: 5 | train_loss: 0.6304 | train_acc: 0.8867 | test_loss: 0.6030 | test_acc: 0.8864\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 2\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e4a4f36a47498fbef5ea9fdc801ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.1084 | train_acc: 0.3047 | test_loss: 0.9680 | test_acc: 0.6705\n",
      "Epoch: 2 | train_loss: 0.8746 | train_acc: 0.7969 | test_loss: 0.8485 | test_acc: 0.7850\n",
      "Epoch: 3 | train_loss: 0.8409 | train_acc: 0.7422 | test_loss: 0.7846 | test_acc: 0.8258\n",
      "Epoch: 4 | train_loss: 0.7511 | train_acc: 0.7383 | test_loss: 0.6660 | test_acc: 0.8873\n",
      "Epoch: 5 | train_loss: 0.6010 | train_acc: 0.9102 | test_loss: 0.6454 | test_acc: 0.8561\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 3\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb3/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6a2c083e3e4e6a9a1ee30f43beba2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0859 | train_acc: 0.3555 | test_loss: 0.9673 | test_acc: 0.7434\n",
      "Epoch: 2 | train_loss: 0.9456 | train_acc: 0.6172 | test_loss: 0.8262 | test_acc: 0.8438\n",
      "Epoch: 3 | train_loss: 0.8497 | train_acc: 0.6836 | test_loss: 0.8040 | test_acc: 0.7850\n",
      "Epoch: 4 | train_loss: 0.7281 | train_acc: 0.7656 | test_loss: 0.7783 | test_acc: 0.7850\n",
      "Epoch: 5 | train_loss: 0.6434 | train_acc: 0.9023 | test_loss: 0.7179 | test_acc: 0.7955\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 4\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36a66c6fce04cb2b8d41b66130feb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0614 | train_acc: 0.4062 | test_loss: 0.9488 | test_acc: 0.4962\n",
      "Epoch: 2 | train_loss: 0.9128 | train_acc: 0.5820 | test_loss: 0.7870 | test_acc: 0.7320\n",
      "Epoch: 3 | train_loss: 0.7612 | train_acc: 0.8828 | test_loss: 0.6915 | test_acc: 0.8551\n",
      "Epoch: 4 | train_loss: 0.6993 | train_acc: 0.7695 | test_loss: 0.6648 | test_acc: 0.8665\n",
      "Epoch: 5 | train_loss: 0.6813 | train_acc: 0.7422 | test_loss: 0.6255 | test_acc: 0.8759\n",
      "Epoch: 6 | train_loss: 0.5800 | train_acc: 0.9141 | test_loss: 0.5140 | test_acc: 0.8864\n",
      "Epoch: 7 | train_loss: 0.5574 | train_acc: 0.8828 | test_loss: 0.4839 | test_acc: 0.8655\n",
      "Epoch: 8 | train_loss: 0.5740 | train_acc: 0.7461 | test_loss: 0.4702 | test_acc: 0.8655\n",
      "Epoch: 9 | train_loss: 0.4660 | train_acc: 0.8945 | test_loss: 0.4831 | test_acc: 0.9072\n",
      "Epoch: 10 | train_loss: 0.5759 | train_acc: 0.7656 | test_loss: 0.5137 | test_acc: 0.8968\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 5\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c79dd3856c8487797d98dafd47a611d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9969 | train_acc: 0.5938 | test_loss: 0.9300 | test_acc: 0.7131\n",
      "Epoch: 2 | train_loss: 0.8888 | train_acc: 0.6328 | test_loss: 0.8635 | test_acc: 0.7027\n",
      "Epoch: 3 | train_loss: 0.7751 | train_acc: 0.7344 | test_loss: 0.7867 | test_acc: 0.8049\n",
      "Epoch: 4 | train_loss: 0.7155 | train_acc: 0.7773 | test_loss: 0.6709 | test_acc: 0.8864\n",
      "Epoch: 5 | train_loss: 0.6367 | train_acc: 0.7695 | test_loss: 0.6599 | test_acc: 0.8466\n",
      "Epoch: 6 | train_loss: 0.6543 | train_acc: 0.7695 | test_loss: 0.6109 | test_acc: 0.8864\n",
      "Epoch: 7 | train_loss: 0.5508 | train_acc: 0.7969 | test_loss: 0.5880 | test_acc: 0.9176\n",
      "Epoch: 8 | train_loss: 0.5442 | train_acc: 0.8008 | test_loss: 0.5694 | test_acc: 0.8977\n",
      "Epoch: 9 | train_loss: 0.4752 | train_acc: 0.9492 | test_loss: 0.5684 | test_acc: 0.8977\n",
      "Epoch: 10 | train_loss: 0.5139 | train_acc: 0.8047 | test_loss: 0.5492 | test_acc: 0.9072\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 6\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Data: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_10_percent/effnetb3/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d5c5ca08544efe88f844f020745b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0876 | train_acc: 0.3750 | test_loss: 1.0211 | test_acc: 0.6098\n",
      "Epoch: 2 | train_loss: 0.9306 | train_acc: 0.6602 | test_loss: 0.9176 | test_acc: 0.6411\n",
      "Epoch: 3 | train_loss: 0.8124 | train_acc: 0.7109 | test_loss: 0.7670 | test_acc: 0.8864\n",
      "Epoch: 4 | train_loss: 0.7401 | train_acc: 0.7227 | test_loss: 0.7217 | test_acc: 0.8561\n",
      "Epoch: 5 | train_loss: 0.6992 | train_acc: 0.7852 | test_loss: 0.6490 | test_acc: 0.8958\n",
      "Epoch: 6 | train_loss: 0.6001 | train_acc: 0.8984 | test_loss: 0.6447 | test_acc: 0.8561\n",
      "Epoch: 7 | train_loss: 0.5729 | train_acc: 0.8516 | test_loss: 0.6518 | test_acc: 0.7850\n",
      "Epoch: 8 | train_loss: 0.5505 | train_acc: 0.8516 | test_loss: 0.6255 | test_acc: 0.8258\n",
      "Epoch: 9 | train_loss: 0.5179 | train_acc: 0.8633 | test_loss: 0.6005 | test_acc: 0.8258\n",
      "Epoch: 10 | train_loss: 0.5640 | train_acc: 0.7695 | test_loss: 0.5707 | test_acc: 0.8561\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 7\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192dcbc5400c4f66870296ae538cc30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9855 | train_acc: 0.5417 | test_loss: 0.7065 | test_acc: 0.8362\n",
      "Epoch: 2 | train_loss: 0.6801 | train_acc: 0.8438 | test_loss: 0.5875 | test_acc: 0.8977\n",
      "Epoch: 3 | train_loss: 0.5614 | train_acc: 0.8854 | test_loss: 0.4828 | test_acc: 0.8561\n",
      "Epoch: 4 | train_loss: 0.4643 | train_acc: 0.9000 | test_loss: 0.4444 | test_acc: 0.8977\n",
      "Epoch: 5 | train_loss: 0.4535 | train_acc: 0.8792 | test_loss: 0.3975 | test_acc: 0.9081\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 8\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849591b023614d029b080f463502615a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0056 | train_acc: 0.5083 | test_loss: 0.8464 | test_acc: 0.7453\n",
      "Epoch: 2 | train_loss: 0.7423 | train_acc: 0.8063 | test_loss: 0.7171 | test_acc: 0.8873\n",
      "Epoch: 3 | train_loss: 0.5745 | train_acc: 0.8625 | test_loss: 0.6168 | test_acc: 0.8977\n",
      "Epoch: 4 | train_loss: 0.5125 | train_acc: 0.8833 | test_loss: 0.5351 | test_acc: 0.8977\n",
      "Epoch: 5 | train_loss: 0.4655 | train_acc: 0.8688 | test_loss: 0.5318 | test_acc: 0.8977\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 9\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb3/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac016d7f9743461997f819ae8fa8cdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9698 | train_acc: 0.5479 | test_loss: 0.7940 | test_acc: 0.7955\n",
      "Epoch: 2 | train_loss: 0.7510 | train_acc: 0.7958 | test_loss: 0.6631 | test_acc: 0.8362\n",
      "Epoch: 3 | train_loss: 0.5741 | train_acc: 0.8792 | test_loss: 0.5623 | test_acc: 0.8968\n",
      "Epoch: 4 | train_loss: 0.5236 | train_acc: 0.8542 | test_loss: 0.5068 | test_acc: 0.8665\n",
      "Epoch: 5 | train_loss: 0.4358 | train_acc: 0.9104 | test_loss: 0.4601 | test_acc: 0.8759\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 10\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb9d4c00c7e424381563316c544f0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9422 | train_acc: 0.6188 | test_loss: 0.6703 | test_acc: 0.9072\n",
      "Epoch: 2 | train_loss: 0.6634 | train_acc: 0.8583 | test_loss: 0.5885 | test_acc: 0.8873\n",
      "Epoch: 3 | train_loss: 0.5510 | train_acc: 0.8833 | test_loss: 0.4699 | test_acc: 0.9280\n",
      "Epoch: 4 | train_loss: 0.4476 | train_acc: 0.9104 | test_loss: 0.4144 | test_acc: 0.9176\n",
      "Epoch: 5 | train_loss: 0.4184 | train_acc: 0.9021 | test_loss: 0.4010 | test_acc: 0.8977\n",
      "Epoch: 6 | train_loss: 0.3909 | train_acc: 0.8938 | test_loss: 0.3495 | test_acc: 0.9176\n",
      "Epoch: 7 | train_loss: 0.3253 | train_acc: 0.9313 | test_loss: 0.3666 | test_acc: 0.9081\n",
      "Epoch: 8 | train_loss: 0.3464 | train_acc: 0.8729 | test_loss: 0.3053 | test_acc: 0.9489\n",
      "Epoch: 9 | train_loss: 0.2974 | train_acc: 0.9292 | test_loss: 0.3100 | test_acc: 0.9072\n",
      "Epoch: 10 | train_loss: 0.3567 | train_acc: 0.8625 | test_loss: 0.2886 | test_acc: 0.9384\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 11\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c683ca4c2a483784fc1ad4530a0ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9487 | train_acc: 0.6604 | test_loss: 0.8240 | test_acc: 0.7746\n",
      "Epoch: 2 | train_loss: 0.7086 | train_acc: 0.7937 | test_loss: 0.6652 | test_acc: 0.9176\n",
      "Epoch: 3 | train_loss: 0.5875 | train_acc: 0.8708 | test_loss: 0.6078 | test_acc: 0.8873\n",
      "Epoch: 4 | train_loss: 0.4774 | train_acc: 0.8938 | test_loss: 0.5441 | test_acc: 0.8864\n",
      "Epoch: 5 | train_loss: 0.4414 | train_acc: 0.8833 | test_loss: 0.5105 | test_acc: 0.8665\n",
      "Epoch: 6 | train_loss: 0.4053 | train_acc: 0.9062 | test_loss: 0.4962 | test_acc: 0.8968\n",
      "Epoch: 7 | train_loss: 0.4010 | train_acc: 0.8750 | test_loss: 0.4649 | test_acc: 0.8977\n",
      "Epoch: 8 | train_loss: 0.3487 | train_acc: 0.9021 | test_loss: 0.4036 | test_acc: 0.9280\n",
      "Epoch: 9 | train_loss: 0.3105 | train_acc: 0.9229 | test_loss: 0.4872 | test_acc: 0.8674\n",
      "Epoch: 10 | train_loss: 0.3427 | train_acc: 0.9313 | test_loss: 0.4503 | test_acc: 0.8873\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 12\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Data: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_20_percent/effnetb3/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b2be1076f7461ab9e5fb9d3dd1438b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9800 | train_acc: 0.5521 | test_loss: 0.8764 | test_acc: 0.7850\n",
      "Epoch: 2 | train_loss: 0.7185 | train_acc: 0.8438 | test_loss: 0.6610 | test_acc: 0.8456\n",
      "Epoch: 3 | train_loss: 0.5985 | train_acc: 0.8688 | test_loss: 0.6072 | test_acc: 0.8456\n",
      "Epoch: 4 | train_loss: 0.4889 | train_acc: 0.8896 | test_loss: 0.5167 | test_acc: 0.8655\n",
      "Epoch: 5 | train_loss: 0.4531 | train_acc: 0.8792 | test_loss: 0.5242 | test_acc: 0.8049\n",
      "Epoch: 6 | train_loss: 0.4569 | train_acc: 0.8688 | test_loss: 0.4649 | test_acc: 0.8352\n",
      "Epoch: 7 | train_loss: 0.4685 | train_acc: 0.8708 | test_loss: 0.4908 | test_acc: 0.8456\n",
      "Epoch: 8 | train_loss: 0.4009 | train_acc: 0.8771 | test_loss: 0.4142 | test_acc: 0.8864\n",
      "Epoch: 9 | train_loss: 0.3868 | train_acc: 0.9042 | test_loss: 0.4575 | test_acc: 0.8153\n",
      "Epoch: 10 | train_loss: 0.3253 | train_acc: 0.9125 | test_loss: 0.4472 | test_acc: 0.8049\n",
      "--------------------------------------------------\n",
      "\n",
      "CPU times: user 1min 15s, sys: 44.2 s, total: 1min 59s\n",
      "Wall time: 2min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TODO: your code\n",
    "experiment_number = 0\n",
    "\n",
    "for data_name, data in dataloaders:\n",
    "    for epochs in epochs_count:\n",
    "        for model_name in model_names:\n",
    "            experiment_number += 1\n",
    "            print(f\"[INFO] Experiment numer: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] Data: {data_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")\n",
    "            \n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0()\n",
    "            elif model_name == \"effnetb2\":\n",
    "                model = create_effnetb2()\n",
    "            else:\n",
    "                model = create_effnetb3()\n",
    "    \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    \n",
    "            train(model=model,\n",
    "                  train_dataloader=data,\n",
    "                  test_dataloader=test_dataloader, \n",
    "                  optimizer=optimizer,\n",
    "                  loss_fn=loss_fn,\n",
    "                  epochs=epochs,\n",
    "                  device=device,\n",
    "                  writer=create_writer(experiment_name=data_name,\n",
    "                                       model_name=model_name,\n",
    "                                       extra=f\"{epochs}_epochs\"))\n",
    "\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}_{data_name}_{epochs}_epochs\")\n",
    "            print(\"-\"*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effnetb0 worked the best, primarly because the bigger models didn't leverage their capability of using bigger images, but also because we didn't train for too long. Data augmentation may also help, which I will investigate in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqlStPo-gbrF"
   },
   "source": [
    "## Exercise 2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n",
    "    \n",
    "* For example, you could have one training DataLoader that uses data augmentation (e.g. `train_dataloader_20_percent_aug` and `train_dataloader_20_percent_no_aug`) and then compare the results of two of the same model types training on these two DataLoaders.\n",
    "* **Note:** You may need to alter the `create_dataloaders()` function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) for examples of using data augmentation or the script below for an example:\n",
    "\n",
    "```python\n",
    "# Note: Data augmentation transform like this should only be performed on training data\n",
    "train_transform_data_aug = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Create a helper function to visualize different augmented (and not augmented) images\n",
    "def view_dataloader_images(dataloader, n=10):\n",
    "    if n > 10:\n",
    "        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n",
    "        n = 10\n",
    "    imgs, labels = next(iter(dataloader))\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    for i in range(n):\n",
    "        # Min max scale the image for display purposes\n",
    "        targ_image = imgs[i]\n",
    "        sample_min, sample_max = targ_image.min(), targ_image.max()\n",
    "        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n",
    "\n",
    "        # Plot images with appropriate axes information\n",
    "        plt.subplot(1, 10, i+1)\n",
    "        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(False)\n",
    "\n",
    "# Have to update `create_dataloaders()` to handle different augmentations\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n",
    "\n",
    "# Note: this is an update version of data_setup.create_dataloaders to handle\n",
    "# differnt train and test transforms.\n",
    "def create_dataloaders(\n",
    "    train_dir, \n",
    "    test_dir, \n",
    "    train_transform, # add parameter for train transform (transforms on train dataset)\n",
    "    test_transform,  # add parameter for test transform (transforms on test dataset)\n",
    "    batch_size=32, num_workers=NUM_WORKERS\n",
    "):\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to update `create_dataloaders()` to handle different augmentations\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n",
    "\n",
    "# Note: this is an update version of data_setup.create_dataloaders to handle\n",
    "# differnt train and test transforms.\n",
    "def create_dataloaders(\n",
    "    train_dir, \n",
    "    test_dir, \n",
    "    train_transform, # add parameter for train transform (transforms on train dataset)\n",
    "    test_transform,  # add parameter for test transform (transforms on test dataset)\n",
    "    batch_size=32, num_workers=NUM_WORKERS\n",
    "):\n",
    "    # Use ImageFolder to create dataset(s)\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    # Turn images into data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I will try all the 3 models with their default input size and data augmentation to see how good can they get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ImageClassification(\n",
       "     crop_size=[224]\n",
       "     resize_size=[256]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BICUBIC\n",
       " ),\n",
       " ImageClassification(\n",
       "     crop_size=[288]\n",
       "     resize_size=[288]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BICUBIC\n",
       " ),\n",
       " ImageClassification(\n",
       "     crop_size=[300]\n",
       "     resize_size=[320]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BICUBIC\n",
       " ))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: your code\n",
    "test_transform0 = torchvision.models.EfficientNet_B0_Weights.DEFAULT.transforms()\n",
    "test_transform2 = torchvision.models.EfficientNet_B2_Weights.DEFAULT.transforms()\n",
    "test_transform3 = torchvision.models.EfficientNet_B3_Weights.DEFAULT.transforms()\n",
    "test_transform0, test_transform2, test_transform3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]),\n",
       " torch.Size([32, 3, 288, 288]),\n",
       " torch.Size([32, 3, 300, 300]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.rand(32, 3, 1000, 1000)\n",
    "test_transform0(batch).shape, test_transform2(batch).shape, test_transform3(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "transform0 = v2.Compose([\n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform2 = v2.Compose([\n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((288, 288)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform3 = v2.Compose([\n",
    "    v2.TrivialAugmentWide(),\n",
    "    v2.Resize((300, 300)),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                 std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "E1N3yyDOoH2t"
   },
   "outputs": [],
   "source": [
    "train_dl0, test_dl0, class_names = create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                     test_dir=test_dir,\n",
    "                                                     train_transform=transform0,\n",
    "                                                     test_transform=test_transform0,\n",
    "                                                     batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dl2, test_dl2, _ = create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                     test_dir=test_dir,\n",
    "                                                     train_transform=transform2,\n",
    "                                                     test_transform=test_transform2,\n",
    "                                                     batch_size=BATCH_SIZE)\n",
    "\n",
    "train_dl3, test_dl3, _ = create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                     test_dir=test_dir,\n",
    "                                                     train_transform=transform3,\n",
    "                                                     test_transform=test_transform3,\n",
    "                                                     batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_count = [10, 20]\n",
    "model_names = [\"effnetb0\", \"effnetb2\", \"effnetb3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment numer: 1\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fada9126e64495a49ba8343dfffdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9880 | train_acc: 0.5479 | test_loss: 0.7592 | test_acc: 0.8049\n",
      "Epoch: 2 | train_loss: 0.7355 | train_acc: 0.7646 | test_loss: 0.5830 | test_acc: 0.8759\n",
      "Epoch: 3 | train_loss: 0.5908 | train_acc: 0.8479 | test_loss: 0.4884 | test_acc: 0.9167\n",
      "Epoch: 4 | train_loss: 0.5481 | train_acc: 0.8125 | test_loss: 0.4681 | test_acc: 0.8561\n",
      "Epoch: 5 | train_loss: 0.5102 | train_acc: 0.8604 | test_loss: 0.3967 | test_acc: 0.8968\n",
      "Epoch: 6 | train_loss: 0.4326 | train_acc: 0.8792 | test_loss: 0.3922 | test_acc: 0.8665\n",
      "Epoch: 7 | train_loss: 0.4787 | train_acc: 0.8313 | test_loss: 0.3548 | test_acc: 0.8864\n",
      "Epoch: 8 | train_loss: 0.4576 | train_acc: 0.8354 | test_loss: 0.3545 | test_acc: 0.8759\n",
      "Epoch: 9 | train_loss: 0.4717 | train_acc: 0.8229 | test_loss: 0.3603 | test_acc: 0.8665\n",
      "Epoch: 10 | train_loss: 0.4028 | train_acc: 0.8542 | test_loss: 0.3100 | test_acc: 0.8665\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 2\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3549352f164e62aef778e00a31134a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9906 | train_acc: 0.5417 | test_loss: 0.7694 | test_acc: 0.8864\n",
      "Epoch: 2 | train_loss: 0.7538 | train_acc: 0.7708 | test_loss: 0.6616 | test_acc: 0.8561\n",
      "Epoch: 3 | train_loss: 0.6527 | train_acc: 0.8313 | test_loss: 0.5771 | test_acc: 0.8665\n",
      "Epoch: 4 | train_loss: 0.5713 | train_acc: 0.8479 | test_loss: 0.5190 | test_acc: 0.8769\n",
      "Epoch: 5 | train_loss: 0.4750 | train_acc: 0.8833 | test_loss: 0.4923 | test_acc: 0.8873\n",
      "Epoch: 6 | train_loss: 0.4326 | train_acc: 0.8854 | test_loss: 0.4356 | test_acc: 0.8769\n",
      "Epoch: 7 | train_loss: 0.4470 | train_acc: 0.8729 | test_loss: 0.4911 | test_acc: 0.8570\n",
      "Epoch: 8 | train_loss: 0.4411 | train_acc: 0.8833 | test_loss: 0.4339 | test_acc: 0.8873\n",
      "Epoch: 9 | train_loss: 0.4165 | train_acc: 0.8958 | test_loss: 0.4096 | test_acc: 0.8873\n",
      "Epoch: 10 | train_loss: 0.4593 | train_acc: 0.7958 | test_loss: 0.4062 | test_acc: 0.8769\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 3\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb3/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539b3d1a94e4996b65817b72d9d4d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0000 | train_acc: 0.5854 | test_loss: 0.8971 | test_acc: 0.7652\n",
      "Epoch: 2 | train_loss: 0.7361 | train_acc: 0.8646 | test_loss: 0.6612 | test_acc: 0.8267\n",
      "Epoch: 3 | train_loss: 0.6036 | train_acc: 0.8771 | test_loss: 0.5338 | test_acc: 0.8873\n",
      "Epoch: 4 | train_loss: 0.5167 | train_acc: 0.8708 | test_loss: 0.4910 | test_acc: 0.9081\n",
      "Epoch: 5 | train_loss: 0.5057 | train_acc: 0.8396 | test_loss: 0.4047 | test_acc: 0.9176\n",
      "Epoch: 6 | train_loss: 0.4185 | train_acc: 0.9083 | test_loss: 0.4002 | test_acc: 0.8977\n",
      "Epoch: 7 | train_loss: 0.4131 | train_acc: 0.9021 | test_loss: 0.3810 | test_acc: 0.8977\n",
      "Epoch: 8 | train_loss: 0.4416 | train_acc: 0.8542 | test_loss: 0.3668 | test_acc: 0.9384\n",
      "Epoch: 9 | train_loss: 0.3707 | train_acc: 0.8979 | test_loss: 0.3044 | test_acc: 0.9280\n",
      "Epoch: 10 | train_loss: 0.3527 | train_acc: 0.8979 | test_loss: 0.2804 | test_acc: 0.9583\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 4\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] Number of epochs: 20\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb0/20_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c887d7277741e396faaa22a243f5b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9777 | train_acc: 0.5729 | test_loss: 0.7809 | test_acc: 0.8352\n",
      "Epoch: 2 | train_loss: 0.7189 | train_acc: 0.7917 | test_loss: 0.5754 | test_acc: 0.8854\n",
      "Epoch: 3 | train_loss: 0.6673 | train_acc: 0.7625 | test_loss: 0.4888 | test_acc: 0.9062\n",
      "Epoch: 4 | train_loss: 0.5098 | train_acc: 0.8625 | test_loss: 0.4813 | test_acc: 0.8769\n",
      "Epoch: 5 | train_loss: 0.4774 | train_acc: 0.8313 | test_loss: 0.3934 | test_acc: 0.8864\n",
      "Epoch: 6 | train_loss: 0.5075 | train_acc: 0.7937 | test_loss: 0.4084 | test_acc: 0.9072\n",
      "Epoch: 7 | train_loss: 0.4824 | train_acc: 0.8083 | test_loss: 0.3481 | test_acc: 0.8968\n",
      "Epoch: 8 | train_loss: 0.4213 | train_acc: 0.8396 | test_loss: 0.4061 | test_acc: 0.8873\n",
      "Epoch: 9 | train_loss: 0.3928 | train_acc: 0.8708 | test_loss: 0.3216 | test_acc: 0.8968\n",
      "Epoch: 10 | train_loss: 0.4191 | train_acc: 0.8500 | test_loss: 0.3243 | test_acc: 0.8864\n",
      "Epoch: 11 | train_loss: 0.4380 | train_acc: 0.8417 | test_loss: 0.3394 | test_acc: 0.8873\n",
      "Epoch: 12 | train_loss: 0.3428 | train_acc: 0.8896 | test_loss: 0.2913 | test_acc: 0.8968\n",
      "Epoch: 13 | train_loss: 0.3340 | train_acc: 0.8688 | test_loss: 0.3241 | test_acc: 0.8873\n",
      "Epoch: 14 | train_loss: 0.3216 | train_acc: 0.9104 | test_loss: 0.2939 | test_acc: 0.9072\n",
      "Epoch: 15 | train_loss: 0.3492 | train_acc: 0.8562 | test_loss: 0.2834 | test_acc: 0.9072\n",
      "Epoch: 16 | train_loss: 0.3392 | train_acc: 0.8958 | test_loss: 0.2742 | test_acc: 0.9072\n",
      "Epoch: 17 | train_loss: 0.3289 | train_acc: 0.8396 | test_loss: 0.2713 | test_acc: 0.9072\n",
      "Epoch: 18 | train_loss: 0.3881 | train_acc: 0.8458 | test_loss: 0.2631 | test_acc: 0.9176\n",
      "Epoch: 19 | train_loss: 0.3270 | train_acc: 0.8667 | test_loss: 0.2716 | test_acc: 0.8873\n",
      "Epoch: 20 | train_loss: 0.3782 | train_acc: 0.8354 | test_loss: 0.2820 | test_acc: 0.8977\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 5\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] Number of epochs: 20\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb2/20_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb3d54983a5406c9884bd3e2e46cd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9609 | train_acc: 0.5813 | test_loss: 0.8185 | test_acc: 0.7140\n",
      "Epoch: 2 | train_loss: 0.7438 | train_acc: 0.7937 | test_loss: 0.6204 | test_acc: 0.9072\n",
      "Epoch: 3 | train_loss: 0.6124 | train_acc: 0.8208 | test_loss: 0.5785 | test_acc: 0.9072\n",
      "Epoch: 4 | train_loss: 0.5026 | train_acc: 0.8854 | test_loss: 0.4841 | test_acc: 0.9176\n",
      "Epoch: 5 | train_loss: 0.4950 | train_acc: 0.8875 | test_loss: 0.4721 | test_acc: 0.9176\n",
      "Epoch: 6 | train_loss: 0.4528 | train_acc: 0.8750 | test_loss: 0.4543 | test_acc: 0.9176\n",
      "Epoch: 7 | train_loss: 0.4304 | train_acc: 0.8417 | test_loss: 0.4265 | test_acc: 0.9280\n",
      "Epoch: 8 | train_loss: 0.3789 | train_acc: 0.9062 | test_loss: 0.4301 | test_acc: 0.9176\n",
      "Epoch: 9 | train_loss: 0.3922 | train_acc: 0.8583 | test_loss: 0.3877 | test_acc: 0.8873\n",
      "Epoch: 10 | train_loss: 0.3502 | train_acc: 0.8771 | test_loss: 0.3423 | test_acc: 0.9176\n",
      "Epoch: 11 | train_loss: 0.3971 | train_acc: 0.8812 | test_loss: 0.4091 | test_acc: 0.9072\n",
      "Epoch: 12 | train_loss: 0.3244 | train_acc: 0.8792 | test_loss: 0.3347 | test_acc: 0.9176\n",
      "Epoch: 13 | train_loss: 0.3312 | train_acc: 0.8792 | test_loss: 0.3689 | test_acc: 0.9176\n",
      "Epoch: 14 | train_loss: 0.3738 | train_acc: 0.8812 | test_loss: 0.3372 | test_acc: 0.9176\n",
      "Epoch: 15 | train_loss: 0.3200 | train_acc: 0.8604 | test_loss: 0.3706 | test_acc: 0.9072\n",
      "Epoch: 16 | train_loss: 0.3073 | train_acc: 0.9062 | test_loss: 0.3576 | test_acc: 0.9072\n",
      "Epoch: 17 | train_loss: 0.2709 | train_acc: 0.9104 | test_loss: 0.3689 | test_acc: 0.8873\n",
      "Epoch: 18 | train_loss: 0.3543 | train_acc: 0.8896 | test_loss: 0.3604 | test_acc: 0.9280\n",
      "Epoch: 19 | train_loss: 0.2893 | train_acc: 0.9292 | test_loss: 0.3864 | test_acc: 0.8769\n",
      "Epoch: 20 | train_loss: 0.2681 | train_acc: 0.9250 | test_loss: 0.3788 | test_acc: 0.9072\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment numer: 6\n",
      "[INFO] Model: effnetb3\n",
      "[INFO] Number of epochs: 20\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-09-14/data_augmented/effnetb3/20_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b541a657d290450195f5ba4f432bcdf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9847 | train_acc: 0.6042 | test_loss: 0.7729 | test_acc: 0.8873\n",
      "Epoch: 2 | train_loss: 0.7594 | train_acc: 0.7979 | test_loss: 0.6441 | test_acc: 0.9176\n",
      "Epoch: 3 | train_loss: 0.6535 | train_acc: 0.8562 | test_loss: 0.5740 | test_acc: 0.8561\n",
      "Epoch: 4 | train_loss: 0.5447 | train_acc: 0.8542 | test_loss: 0.4881 | test_acc: 0.8873\n",
      "Epoch: 5 | train_loss: 0.5170 | train_acc: 0.8875 | test_loss: 0.4455 | test_acc: 0.8977\n",
      "Epoch: 6 | train_loss: 0.4397 | train_acc: 0.8958 | test_loss: 0.4528 | test_acc: 0.8769\n",
      "Epoch: 7 | train_loss: 0.3818 | train_acc: 0.9104 | test_loss: 0.3971 | test_acc: 0.8873\n",
      "Epoch: 8 | train_loss: 0.3976 | train_acc: 0.8854 | test_loss: 0.3584 | test_acc: 0.8873\n",
      "Epoch: 9 | train_loss: 0.3716 | train_acc: 0.9167 | test_loss: 0.4005 | test_acc: 0.8769\n",
      "Epoch: 10 | train_loss: 0.3380 | train_acc: 0.9104 | test_loss: 0.3192 | test_acc: 0.8769\n",
      "Epoch: 11 | train_loss: 0.3435 | train_acc: 0.9187 | test_loss: 0.3273 | test_acc: 0.8873\n",
      "Epoch: 12 | train_loss: 0.4014 | train_acc: 0.8667 | test_loss: 0.3231 | test_acc: 0.9081\n",
      "Epoch: 13 | train_loss: 0.3319 | train_acc: 0.9187 | test_loss: 0.2574 | test_acc: 0.9176\n",
      "Epoch: 14 | train_loss: 0.3612 | train_acc: 0.9000 | test_loss: 0.3224 | test_acc: 0.8977\n",
      "Epoch: 15 | train_loss: 0.3864 | train_acc: 0.9021 | test_loss: 0.2510 | test_acc: 0.9280\n",
      "Epoch: 16 | train_loss: 0.2709 | train_acc: 0.9229 | test_loss: 0.2992 | test_acc: 0.8873\n",
      "Epoch: 17 | train_loss: 0.2952 | train_acc: 0.9208 | test_loss: 0.2783 | test_acc: 0.8977\n",
      "Epoch: 18 | train_loss: 0.2771 | train_acc: 0.9021 | test_loss: 0.2741 | test_acc: 0.9081\n",
      "Epoch: 19 | train_loss: 0.2697 | train_acc: 0.9187 | test_loss: 0.2500 | test_acc: 0.8873\n",
      "Epoch: 20 | train_loss: 0.2920 | train_acc: 0.9062 | test_loss: 0.2788 | test_acc: 0.9081\n",
      "--------------------------------------------------\n",
      "\n",
      "CPU times: user 2min 28s, sys: 52 s, total: 3min 20s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "experiment_number = 0\n",
    "\n",
    "for epochs in epochs_count:\n",
    "    for model_name in model_names:\n",
    "        experiment_number += 1\n",
    "        print(f\"[INFO] Experiment numer: {experiment_number}\")\n",
    "        print(f\"[INFO] Model: {model_name}\")\n",
    "        print(f\"[INFO] Number of epochs: {epochs}\")\n",
    "        \n",
    "        if model_name == \"effnetb0\":\n",
    "            model = create_effnetb0()\n",
    "            train_dataloader = train_dl0\n",
    "            test_dataloader = test_dl0\n",
    "        elif model_name == \"effnetb2\":\n",
    "            model = create_effnetb2()\n",
    "            train_dataloader = train_dl2\n",
    "            test_dataloader = test_dl2\n",
    "        else:\n",
    "            model = create_effnetb3()\n",
    "            train_dataloader = train_dl3\n",
    "            test_dataloader = test_dl3\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "        train(model=model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              test_dataloader=test_dataloader,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=loss_fn,\n",
    "              epochs=epochs,\n",
    "              device=device,\n",
    "              writer=create_writer(experiment_name=\"data_augmented\",\n",
    "                                   model_name=model_name,\n",
    "                                   extra=f\"{epochs}_epochs\"))\n",
    "\n",
    "        torch.save(model.state_dict(), f\"models/{model_name}_augmented_20_data_{epochs}_epochs\")\n",
    "        print(\"-\"*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the augmentation didn't make a noticable difference, training for 20 instead of 10 epochs also seems pointless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IvuTskxgjaw"
   },
   "source": [
    "## Exercise 3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire [Food101 dataset from `torchvision.models`](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101)\n",
    "    \n",
    "* You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\n",
    "* If you try more than one model, it would be good to have the model's results tracked.\n",
    "* If you load the Food101 dataset from `torchvision.models`, you'll have to create PyTorch DataLoaders to use it in training.\n",
    "* **Note:** Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have done this (and much more) here** https://github.com/noNScop/sem2.5/tree/main/image_classification_on_food101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4+Nwb43yrG43qNz11d5C4",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "07_pytorch_experiment_tracking_exercise_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
